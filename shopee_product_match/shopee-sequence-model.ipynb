{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.python.keras.utils.vis_utils import plot_model\nfrom tensorflow.keras.utils import Progbar\nfrom datetime import datetime\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parameter","metadata":{}},{"cell_type":"code","source":"batch_size = 64\nexecution_mode = 'submission'\nlearning_rate = 0.0001\noov_token = '<oov>'\nnum_words = 20000\ntruncating = 'post'\npadding = 'post'\nembedding_dim = 50\nmax_len = 20\nepochs = 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data cleaning","metadata":{}},{"cell_type":"code","source":"def clean_data(x):\n    x = x.replace('!', '')\n    x = x.replace('\"', '')\n    x = x.replace('#', '')\n    x = x.replace('$', '')\n    x = x.replace('%', '')\n    x = x.replace('&', '')\n    x = x.replace(\"'\", '')\n    x = x.replace('(', '')\n    x = x.replace(')', '')\n    x = x.replace('*', '')\n    x = x.replace('+', '')\n    x = x.replace(',', '')\n    x = x.replace('-', '')\n    x = x.replace('.', '')\n    x = x.replace('/', '')\n    x = x.replace('0', '')\n    x = x.replace('1', '')\n    x = x.replace('2', '')\n    x = x.replace('3', '')\n    x = x.replace('4', '')\n    x = x.replace('5', '')\n    x = x.replace('6', '')\n    x = x.replace('7', '')\n    x = x.replace('8', '')\n    x = x.replace('9', '')\n    x = x.replace(':', '')\n    x = x.replace(';', '')\n    x = x.replace('<', '')\n    x = x.replace('=', '')\n    x = x.replace('>', '')\n    x = x.replace('?', '')\n    x = x.replace('@', '')\n    x = x.replace('[', '')\n    x = x.replace('\\\\', '')\n    x = x.replace(']', '')\n    x = x.replace('^', '')\n    x = x.replace('_', '')\n    x = x.replace('`', '')\n    x = x.replace('{', '')\n    x = x.replace('|', '')\n    x = x.replace('}', '')\n    x = x.replace('~', '')\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_all_char(df, col_name):\n    all_char = []\n    for index, row in df.iterrows():\n        for char in row[col_name]:\n            all_char.append(char)\n    return set(all_char)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/shopee-product-matching/train.csv')\ntrain_data['title'] = train_data['title'].apply(clean_data)\n#train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data prep","metadata":{}},{"cell_type":"code","source":"def create_pair(train_data):\n  pos_pair = []\n  neg_pair = []\n  train_data = train_data\n  all_label_group = set(train_data['label_group'])\n  for label in all_label_group:\n    #Positive pair\n    pos_list = train_data[train_data['label_group'] == label]['posting_id'].values\n    for i in range(len(pos_list)-1):\n      pos_pair.append([pos_list[i], pos_list[i+1], 1])\n    #Negative pair\n    neg_label = random.sample(all_label_group - set([label]), 1)[0]\n    neg_list = train_data[train_data['label_group'] == neg_label]['posting_id'].values\n    n = min(len(pos_list), len(neg_list))\n    for i in range(n):\n      neg_pair.append([pos_list[i], neg_list[i], 0])\n  #Merge pos and neg pair\n  all_pairs = pos_pair+ neg_pair\n  #Create DataaFrame\n  df_all_pairs = pd.DataFrame(data=all_pairs, columns=['posting_id1','posting_id2','label'])\n  df_all_pairs = pd.merge(df_all_pairs, train_data, left_on='posting_id1', right_on='posting_id', how='inner')\n  df_all_pairs = df_all_pairs[['posting_id1', 'posting_id2', 'image', 'title', 'label']]\n  df_all_pairs = df_all_pairs.rename(columns={'image':'image1', 'title':'title1'})\n  df_all_pairs = pd.merge(df_all_pairs, train_data, left_on='posting_id2', right_on='posting_id', how='inner')\n  df_all_pairs = df_all_pairs[['posting_id1', 'posting_id2','image1','title1','image', 'title', 'label']]\n  df_all_pairs = df_all_pairs.rename(columns={'image':'image2', 'title':'title2'})\n  return df_all_pairs\n\ndef get_true_matches(train_data):\n    all_matches = []\n    for index, row in train_data.iterrows():\n        posting_id1 = row['posting_id']\n        matching_postings = train_data[train_data['label_group'] == row['label_group']]['posting_id'].values\n        matching_postings = ' '.join(x for x in matching_postings)\n        all_matches.append([posting_id1,matching_postings])\n    return pd.DataFrame(data=all_matches, columns=['posting_id', 'matches'])\n\ndef get_f1_score(actual, predicted):\n    f_score = []\n    for index,row in actual.iterrows():\n        id = row['posting_id']\n        list = row['matches'].split()\n        pred_list = predicted[predicted['posting_id'] == id]['matches'].values[0].split()\n        ##F1 score\n        tags = set(list)\n        pred = set(pred_list)\n        tp = len(tags & pred)\n        fp = len(pred) - tp \n        fn = len(tags) - tp\n        if tp>0:\n            precision=float(tp)/(tp+fp)\n            recall=float(tp)/(tp+fn)\n            f_score.append(2*((precision*recall)/(precision+recall)))\n        else:\n            f_score.append(0)\n    return np.array(f_score).mean()\n\ndef create_sequence(data, col):\n    seq = np.array(pad_sequences(tokenizer.texts_to_sequences([x for x in data[col].values]), maxlen=max_len, \n              truncating=truncating, padding=padding))\n    return seq\n\n#Learn Tokenizer for sequence\ntrain_data = train_data\ntitles = []\nfor index, row in train_data.iterrows():\n    titles.append(row['title'])\ntokenizer = Tokenizer(oov_token=oov_token, num_words=num_words)\ntokenizer.fit_on_texts(titles)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Batched Dataset","metadata":{}},{"cell_type":"code","source":"if execution_mode == 'train-test' or execution_mode == 'train-test':\n    paired_train_data = create_pair(train_data)\n    training_data, testing_data = train_test_split(paired_train_data, test_size=0.1)\n    print(\"Total number of Training records: {}\".format(len(training_data['posting_id1'].values)))\n    print(\"Total number of validation records: {}\".format(len(testing_data['posting_id1'].values)))\n    #Training Data Gen\n    dataset = tf.data.Dataset.from_tensor_slices((create_sequence(training_data,'title1'),\n                                                  create_sequence(training_data,'title2'), training_data['label']))\n    dataset = dataset.batch(batch_size).prefetch(1)\n    #Validation Data Gen\n    val_dataset = tf.data.Dataset.from_tensor_slices((create_sequence(testing_data,'title1'),\n                                                  create_sequence(testing_data,'title2'), testing_data['label']))\n    val_dataset = val_dataset.batch(batch_size).prefetch(1)\nelif execution_mode == 'train':\n    paired_train_data = create_pair(train_data)\n    training_data = paired_train_data\n    print(\"Total number of Training records: {}\".format(len(training_data['posting_id1'].values)))\n    #Training Data Gen\n    dataset = tf.data.Dataset.from_tensor_slices((create_sequence(training_data,'title1'),\n                                                  create_sequence(training_data,'title2'), training_data['label']))\n    dataset = dataset.batch(batch_size).prefetch(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"def base_model():\n    #For Image Data\n    input_seq = tf.keras.layers.Input(shape=(max_len), name='base_seq_input')\n    x = tf.keras.layers.Embedding(input_dim=num_words+1, output_dim=embedding_dim,input_length=max_len)(input_seq)\n    x = tf.keras.layers.GRU(256, return_sequences=True)(x)\n    x = tf.keras.layers.SimpleRNN(128)(x)\n    #x = tf.keras.layers.GlobalAveragePooling1D()(x)\n    #x = tf.keras.layers.Dense(32, activation='relu')(x)\n    #x = tf.keras.layers.Dropout(0.2)(x)\n    #x = tf.keras.layers.Dense(64, activation='relu')(x)\n    #x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    #define model\n    model = Model(inputs=input_seq, outputs=x)\n    return model\ndef euclidean_distance(vects):\n    x, y = vects\n    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n    return K.sqrt(K.maximum(sum_square, K.epsilon()))\ndef eucl_dist_output_shape(shapes):\n    shape1, shape2 = shapes\n    return (shape1[0], 1)\ndef contrastive_loss_with_margin(margin):\n    def contrastive_loss(y_true, y_pred):\n        square_pred = K.square(y_pred)\n        margin_square = K.square(K.maximum(margin - y_pred, 0))\n        return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n    return contrastive_loss\n#Define Model\nbase_network = base_model()\n\ninput_a = tf.keras.layers.Input(shape=(max_len), name='input_a')\nvec_output_a = base_network(input_a)\n\ninput_b = tf.keras.layers.Input(shape=(max_len), name='input_b')\nvec_output_b = base_network(input_b)\noutput = tf.keras.layers.Lambda(euclidean_distance, name='output_layer', \n                                    output_shape=eucl_dist_output_shape)([vec_output_a, vec_output_b])\nmy_model = Model(inputs=[input_a, input_b], outputs=output)\n#plot_model(base_network, show_shapes=True, show_layer_names=True, to_file='base-model.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# optimizer and loss","metadata":{}},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\nloss = contrastive_loss_with_margin(1)\nmy_model.compile(optimizer=optimizer, loss=loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_model(my_model, dataset, optimizer, loss):\n    with tf.GradientTape() as tap:\n        input_a = dataset[0]\n        input_b = dataset[1]\n        label = tf.expand_dims(tf.cast(dataset[2], tf.float32), axis=0)\n        output = my_model([input_a, input_b])\n        loss_value = loss(label, output)\n    gradients = tap.gradient(loss_value, my_model.trainable_weights)\n    optimizer.apply_gradients(zip(gradients, my_model.trainable_weights))\n    return loss_value\n\ndef train_data_for_one_epoch(dataset, my_model, optimizer, loss):\n    losses = []\n    for step, single_batch in enumerate(dataset):\n        time.sleep(0.3)\n        loss_value = train_model(my_model, single_batch, optimizer, loss)\n        losses.append(loss_value.numpy())\n        pb_i.add(batch_size)\n    return np.mean(losses)\n\ndef perform_validation(my_model, val_dataset):\n    val_loss = []\n    for step, single_batch in enumerate(val_dataset):\n        input_a = single_batch[0]\n        input_b = single_batch[1]\n        y_pred = my_model([input_a, input_b])\n        val_loss.append(loss(tf.expand_dims(tf.cast(single_batch[2], tf.float32), axis=0), y_pred))\n    return np.mean(val_loss)\n\n#Custom Training\nif execution_mode == 'train-test' or execution_mode == 'train':\n    epoch = range(epochs)\n    num_training_samples = training_data['title1'].count()\n    for epc in epoch:\n        pb_i = Progbar(num_training_samples)\n        epc = epc + 1\n        start_time = datetime.now()\n        #Train model for one epoch\n        losses = train_data_for_one_epoch(dataset, my_model, optimizer, loss)\n        val_loss = perform_validation(my_model, val_dataset)\n        #val_loss = 0\n        #Train End time for one epoch\n        end_time = datetime.now()\n        time_taken_for_one_epoch = (end_time-start_time).total_seconds()\n        print('\\n Epoch %s/%s time taken: %.4f: Train loss: %.4f  Validation Loss: %.4f,' % \\\n          (epc,epochs, float(time_taken_for_one_epoch), float(losses), float(val_loss)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# save model","metadata":{}},{"cell_type":"code","source":"if execution_mode == 'train-test' or execution_mode == 'train':\n    base_network.save('base_network_seq.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"#Read model for inference\nif execution_mode == 'train-test' or execution_mode == 'train':\n    base_network = tf.keras.models.load_model('base_network_seq.h5')\nelse:\n    base_network = tf.keras.models.load_model('../input/shopee-product-matching-seq-final/base_network_seq.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_prediction(data, thld):\n    weight_metrix = base_network.layers[1].get_weights()[0]\n    sequences = create_sequence(data, 'title')\n    pred_vecs = []\n    for seq in sequences:\n        i = 0\n        emb = np.zeros((embedding_dim))\n        for idx in seq:\n            if idx != 0:\n                emb = emb + np.array(weight_metrix[idx])\n                i = i + 1\n        emb = emb/i\n        pred_vecs.append(emb)\n    all_posting_ids = data['posting_id'].values\n    iter1 = 0\n    all_matches = []\n    for vec1 in pred_vecs:\n        vec1 = np.expand_dims(vec1, axis=0)\n        dist = np.sqrt((np.square(vec1[:,np.newaxis]-pred_vecs).sum(axis=2)))\n        match_indices = np.where(dist[0] < thld)[0]\n        res_list = [all_posting_ids[i] for i in match_indices]\n        matches = ' '.join(x for x in res_list)\n        all_matches.append([all_posting_ids[iter1], matches])\n        iter1 = iter1 + 1\n    return pd.DataFrame(data=all_matches, columns=['posting_id', 'matches'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if execution_mode == 'train-test' or execution_mode == 'train':\n    id1 = testing_data['posting_id1'].unique()\n    id2 = testing_data['posting_id2'].unique()\n    ids = set(np.concatenate((id1, id2), axis=None))\n    val_df = pd.DataFrame(data=ids, columns=['lkp_posting_id'])\n    val_df = pd.merge(train_data, val_df, left_on=['posting_id'], right_on=['lkp_posting_id'], \n                      how='inner')['label_group'].unique()\n    val_df = pd.DataFrame(data=val_df, columns=['lkp_label_group'])\n    val_df = pd.merge(train_data, val_df, left_on=['label_group'], right_on=['lkp_label_group'], how='inner')\n    prediction = make_prediction(val_df, .057)\n    true_matches = get_true_matches(val_df)\n    print(\"Validation F1 Score: {}\".format(get_f1_score(true_matches, prediction)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#learning rate --default\n#.08-0.5173479313729755\n#learning rate --.0001\n#.03--0.45473600943616843\n#.04-0.48570234724580486\n#.05--0.5267134612449069\n#.055-- 0.5336425349313603\n#0.057--0.5282102495100531\n#.06--0.5101598238049183\n#.08--0.22476086841316661\n#.1--0.05536571180914155","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/shopee-product-matching/test.csv')\nsubmission_df = make_prediction(test_data, 0.055)\nsubmission_df.to_csv('./submission.csv', index=False)\n#submission_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}