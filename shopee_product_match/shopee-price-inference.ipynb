{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parameter","metadata":{}},{"cell_type":"code","source":"img_height = 75\nimg_width = 75\nbatch_size = 64\noov_token = '<oov>'\nnum_words = 20000\ntruncating = 'post'\npadding = 'post'\nembedding_dim = 50\nmax_len = 20","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Prep","metadata":{}},{"cell_type":"code","source":"def create_sequence(data, col):\n    seq = np.array(pad_sequences(tokenizer.texts_to_sequences([x for x in data[col].values]), maxlen=max_len, \n              truncating=truncating, padding=padding))\n    return seq\ndef clean_data(x):\n    x = x.replace('!', '')\n    x = x.replace('\"', '')\n    x = x.replace('#', '')\n    x = x.replace('$', '')\n    x = x.replace('%', '')\n    x = x.replace('&', '')\n    x = x.replace(\"'\", '')\n    x = x.replace('(', '')\n    x = x.replace(')', '')\n    x = x.replace('*', '')\n    x = x.replace('+', '')\n    x = x.replace(',', '')\n    x = x.replace('-', '')\n    x = x.replace('.', '')\n    x = x.replace('/', '')\n    x = x.replace('0', '')\n    x = x.replace('1', '')\n    x = x.replace('2', '')\n    x = x.replace('3', '')\n    x = x.replace('4', '')\n    x = x.replace('5', '')\n    x = x.replace('6', '')\n    x = x.replace('7', '')\n    x = x.replace('8', '')\n    x = x.replace('9', '')\n    x = x.replace(':', '')\n    x = x.replace(';', '')\n    x = x.replace('<', '')\n    x = x.replace('=', '')\n    x = x.replace('>', '')\n    x = x.replace('?', '')\n    x = x.replace('@', '')\n    x = x.replace('[', '')\n    x = x.replace('\\\\', '')\n    x = x.replace(']', '')\n    x = x.replace('^', '')\n    x = x.replace('_', '')\n    x = x.replace('`', '')\n    x = x.replace('{', '')\n    x = x.replace('|', '')\n    x = x.replace('}', '')\n    x = x.replace('~', '')\n    return x\n\n#Learn Tokenizer for sequence\ntrain_data = pd.read_csv('../input/shopee-product-matching/train.csv')\ntrain_data['title'] = train_data['title'].apply(clean_data)\ntitles = []\nfor index, row in train_data.iterrows():\n    titles.append(row['title'])\ntokenizer = Tokenizer(oov_token=oov_token, num_words=num_words)\ntokenizer.fit_on_texts(titles)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load model","metadata":{}},{"cell_type":"code","source":"base_network_cnn = tf.keras.models.load_model('../input/shopee-base-network-cnn-final/base_network_cnn.h5')\nbase_network_seq = tf.keras.models.load_model('../input/shopee-product-matching-seq-final/base_network_seq.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Prediction","metadata":{}},{"cell_type":"code","source":"def process_test_images(image):\n    dct = {}\n    image_path = tf.constant('/kaggle/input/shopee-product-matching/test_images/')\n    #Process Image\n    img = tf.strings.join([image_path, image])\n    img = tf.io.read_file(img)\n    img = tf.image.decode_jpeg(img)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    img = tf.image.resize(img, [img_height, img_width])\n    dct['image'] = img\n    return dct\ndef make_prediction_cnn(data, thld):\n    pred_dataset = tf.data.Dataset.from_tensor_slices((data['image']))\n    pred_dataset = pred_dataset.map(process_test_images, num_parallel_calls=tf.data.AUTOTUNE)\n    pred_dataset = pred_dataset.batch(batch_size).prefetch(1)\n    pred_vecs = []\n    for step, single_batch in enumerate(pred_dataset):\n        if step == 0:\n            pred_vecs = base_network_cnn.predict(single_batch['image'])\n        else:\n            pred_vec = base_network_cnn.predict(single_batch['image'])\n            pred_vecs = np.concatenate((pred_vecs,pred_vec), axis=0)\n    all_posting_ids = data['posting_id'].values\n    iter1 = 0\n    all_matches = []\n    for vec1 in pred_vecs:\n        vec1 = np.expand_dims(vec1, axis=0)\n        dist = np.sqrt((np.square(vec1[:,np.newaxis]-pred_vecs).sum(axis=2)))\n        match_indices = np.where(dist[0] < thld)[0]\n        res_list = [all_posting_ids[i] for i in match_indices]\n        matches = ' '.join(x for x in res_list)\n        all_matches.append([all_posting_ids[iter1], matches])\n        iter1 = iter1 + 1\n    return pd.DataFrame(data=all_matches, columns=['posting_id', 'matches'])\n\ndef make_prediction_seq(data, thld):\n    weight_metrix = base_network_seq.layers[1].get_weights()[0]\n    sequences = create_sequence(data, 'title')\n    pred_vecs = []\n    for seq in sequences:\n        i = 0\n        emb = np.zeros((embedding_dim))\n        for idx in seq:\n            if idx != 0:\n                emb = emb + np.array(weight_metrix[idx])\n                i = i + 1\n        emb = emb/i\n        pred_vecs.append(emb)\n    all_posting_ids = data['posting_id'].values\n    iter1 = 0\n    all_matches = []\n    for vec1 in pred_vecs:\n        vec1 = np.expand_dims(vec1, axis=0)\n        dist = np.sqrt((np.square(vec1[:,np.newaxis]-pred_vecs).sum(axis=2)))\n        match_indices = np.where(dist[0] < thld)[0]\n        res_list = [all_posting_ids[i] for i in match_indices]\n        matches = ' '.join(x for x in res_list)\n        all_matches.append([all_posting_ids[iter1], matches])\n        iter1 = iter1 + 1\n    return pd.DataFrame(data=all_matches, columns=['posting_id', 'matches'])\n\ndef merge_prediction(cnn_prediction, seq_prediction):\n    final_result = []\n    for index, row in cnn_prediction.iterrows():\n        all_matches = []\n        res = []\n        posting_id = row['posting_id']\n        cnn_matches = row['matches'].split(' ')\n        seq_matches = seq_prediction[seq_prediction['posting_id'] == posting_id]['matches'].values[0].split(' ')\n        for x in cnn_matches:\n            all_matches.append(x)\n        for x in seq_matches:\n            all_matches.append(x)\n        all_matches = list(set(all_matches))\n        res.append(' '.join(x for x in all_matches))\n        final_result.append([posting_id, res[0]])\n    return pd.DataFrame(data=final_result, columns=['posting_id', 'matches'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/shopee-product-matching/test.csv')\ncnn_prediction = make_prediction_cnn(test_data, 0.08)\nseq_prediction = make_prediction_seq(test_data, 0.02)\nsubmission_df = merge_prediction(cnn_prediction, seq_prediction)\nsubmission_df.to_csv('./submission.csv', index=False)\n#submission_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"def get_true_matches(train_data):\n    all_matches = []\n    for index, row in train_data.iterrows():\n        posting_id1 = row['posting_id']\n        matching_postings = train_data[train_data['label_group'] == row['label_group']]['posting_id'].values\n        matching_postings = ' '.join(x for x in matching_postings)\n        all_matches.append([posting_id1,matching_postings])\n    return pd.DataFrame(data=all_matches, columns=['posting_id', 'matches'])\n\ndef get_f1_score(actual, predicted):\n    f_score = []\n    for index,row in actual.iterrows():\n        id = row['posting_id']\n        list = row['matches'].split()\n        pred_list = predicted[predicted['posting_id'] == id]['matches'].values[0].split()\n        ##F1 score\n        tags = set(list)\n        pred = set(pred_list)\n        tp = len(tags & pred)\n        fp = len(pred) - tp \n        fn = len(tags) - tp\n        if tp>0:\n            precision=float(tp)/(tp+fp)\n            recall=float(tp)/(tp+fn)\n            f_score.append(2*((precision*recall)/(precision+recall)))\n        else:\n            f_score.append(0)\n    return np.array(f_score).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ntraining_data, testing_data = train_test_split(train_data, test_size=0.1)\nid1 = testing_data['label_group'].unique()\nval_df = pd.DataFrame(data=id1, columns=['lkp_label_group'])\nval_df = pd.merge(train_data, val_df, left_on=['label_group'], right_on=['lkp_label_group'], \n                      how='inner')\ncnn_prediction = make_prediction_cnn(val_df, 0.14)\nseq_prediction = make_prediction_seq(val_df, 0.055)\nprediction = merge_prediction(cnn_prediction, seq_prediction)\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#true_matches = get_true_matches(val_df)\n#print(\"Validation F1 Score: {}\".format(get_f1_score(true_matches, prediction)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#.11,.055--0.6119074216242151","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}